<div align="center">

Português do Brasil | English

</div>

🚀 Português do Brasil
<div align="center">

Uma plataforma de IA desktop, local e privada. Execute, gira e converse com modelos de linguagem open-source com total controlo e performance.

📥 Baixar a Última Versão • 🐛 Reportar um Bug • 💬 Iniciar uma Discussão

</div>

✨ Funcionalidades Principais
Multi-Backend Flexível: Escolha como executar os seus modelos:

🤖 SevenX Engine (Local): Motor de inferência integrado para modelos Hugging Face, otimizado para rodar localmente em CPU e GPU (NVIDIA).

🦙 Integração com Ollama: Conecte-se ao seu servidor Ollama para usar qualquer modelo da sua biblioteca, incluindo os quantizados (GGUF).

⚡ Performance Otimizada:

Streaming em Tempo Real: Receba respostas do modelo palavra por palavra para uma experiência de chat fluida.

Modo Leve: Ative para reduzir o consumo de CPU em computadores com menos recursos.

Gestão de Memória Inteligente: Apenas um modelo fica ativo na memória por vez, economizando RAM.

🤗 Gestor de Modelos Hugging Face:

Pesquise, baixe e gira milhares de modelos diretamente do Hugging Face Hub.

Suporte para modelos protegidos através de token de acesso.

📊 Monitor de Sistema Completo:

Acompanhe o uso de CPU, RAM e Disco.

Monitoramento de GPU NVIDIA: Veja o uso de VRAM, a utilização e a temperatura da sua placa de vídeo.

🔒 Privacidade Total: Tudo roda na sua máquina. Os seus dados, prompts e conversas nunca saem do seu computador.

🎨 Interface Moderna e Configurável:

UI responsiva construída com PyQt6.

Personalize parâmetros de geração, tema visual, e mais na aba de configurações.

🛠️ Instalação
Windows (Método Rápido)
# 1. Instala as dependências num ambiente virtual
install.bat

# 2. Executa a aplicação
run.bat

Manual (Para todos os sistemas)
# 1. Clone o repositório
git clone [https://github.com/sevenx777-dev/SevenX-Studio.git](https://github.com/sevenx777-dev/SevenX-Studio.git)
cd SevenX-Studio

# 2. Crie e ative um ambiente virtual
python -m venv venv
# No Windows:
venv\Scripts\activate
# No Linux/macOS:
# source venv/bin/activate

# 3. Instale as dependências
pip install -r requirements.txt

# 4. Execute a aplicação
python main.py

🎯 Como Usar
Execute a Aplicação: Inicie o SevenX Studio.

Escolha o Serviço: Na aba "Chat", selecione se quer usar os modelos locais (SevenX (Local)) ou os do Ollama.

Selecione um Modelo:

Se escolheu Ollama, a lista mostrará os modelos que você já tem no Ollama.

Se escolheu SevenX (Local), vá para a aba "Modelos" para pesquisar e baixar modelos do Hugging Face.

Converse: Volte para a aba "Chat", digite a sua mensagem e pressione Enter!

🤝 Contribuindo
Contribuições são muito bem-vindas! Se tem uma ideia ou encontrou um bug, por favor, abra uma Issue ou um Pull Request.

📄 Licença
Este projeto está licenciado sob a Licença MIT. Veja o ficheiro LICENSE para mais detalhes.

🚀 English
<div align="center">

A modern, local, and private desktop AI platform. Run, manage, and chat with open-source language models with full control and performance.

📥 Download Latest Release • 🐛 Report a Bug • 💬 Start a Discussion

</div>

✨ Key Features
Flexible Multi-Backend: Choose how to run your models:

🤖 SevenX Engine (Local): Integrated inference engine for Hugging Face models, optimized to run locally on CPU and GPU (NVIDIA).

🦙 Ollama Integration: Connect to your Ollama server to use any model from its library, including quantized models (GGUF).

⚡ Optimized Performance:

Real-Time Streaming: Receive model responses word-by-word for a fluid chat experience.

Lite Mode: Enable to reduce CPU consumption on less powerful computers.

Smart Memory Management: Only one model remains active in memory at a time, saving RAM.

🤗 Hugging Face Model Manager:

Search, download, and manage thousands of models directly from the Hugging Face Hub.

Support for gated models via access token.

📊 Comprehensive System Monitor:

Track CPU, RAM, and Disk usage.

NVIDIA GPU Monitoring: View VRAM usage, utilization, and the temperature of your graphics card.

🔒 Total Privacy: Everything runs on your machine. Your data, prompts, and conversations never leave your computer.

🎨 Modern and Configurable UI:

Responsive UI built with PyQt6.

Customize generation parameters, visual theme, and more in the settings tab.

🛠️ Installation
Windows (Quick Method)
# 1. Installs dependencies in a virtual environment
install.bat

# 2. Runs the application
run.bat

Manual (All Systems)
# 1. Clone the repository
git clone [https://github.com/sevenx777-dev/SevenX-Studio.git](https://github.com/sevenx777-dev/SevenX-Studio.git)
cd SevenX-Studio

# 2. Create and activate a virtual environment
python -m venv venv
# On Windows:
venv\Scripts\activate
# On Linux/macOS:
# source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Run the application
python main.py

🎯 How to Use
Run the Application: Start SevenX Studio.

Choose the Service: In the "Chat" tab, select whether you want to use local models (SevenX (Local)) or those from Ollama.

Select a Model:

If you chose Ollama, the list will show the models you already have in Ollama.

If you chose SevenX (Local), go to the "Models" tab to search for and download models from Hugging Face.

Chat: Return to the "Chat" tab, type your message, and press Enter!

🤝 Contributing
Contributions are very welcome! If you have an idea or have found a bug, please open an Issue or a Pull Request.

📄 License
This project is licensed under the MIT License. See the LICENSE file for details.
